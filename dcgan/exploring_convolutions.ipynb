{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [`conv1d`](http://pytorch.org/docs/stable/nn.html#torch.nn.functional.conv1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `inputs`: (`batch_size`, `in_channels`, `data_length`)\n",
    "* `filters`: (`out_channels`, `in_channels`, `filter_length`)\n",
    "\n",
    "Default: \n",
    "\n",
    "* No padding\n",
    "* Stride 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(20, 16, 50)\n",
    "filters = torch.randn(33, 16, 3)\n",
    "output = F.conv1d(inputs, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 33, 48])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: \n",
    "\n",
    "1. The filters are of length 3. We want 33 output channels, and we currently have 16 input channels.\n",
    "2. The first output channel is created by sliding one filter over each input channel and summing them up.\n",
    "3. This is then done 32 more times. Thus, we end up with 33 output channels.\n",
    "4. We can repeat this process for each image in our input batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [`conv2d`](http://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `inputs`: (`batch_size`, `in_channels`, `size_h`, `size_w`)\n",
    "* `filters`: (`out_channels`, `in_channels`, `size_h`, `size_w`)\n",
    "\n",
    "Default: \n",
    "\n",
    "* No padding\n",
    "* Stride 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(1,4,5,5)\n",
    "filters = torch.randn(8,4,3,3)\n",
    "output = F.conv2d(inputs, filters, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: \n",
    "\n",
    "1. The filters are of size 3x3. We want 8 output channels, and we currently have 4 input channels.\n",
    "2. The first output channel is created by sliding one filter over each of the 4 input channels and summing them up.\n",
    "3. This is then done 7 more times. Thus, we end up with 8 output channels.\n",
    "4. We can repeat this process for each image in our input batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rules of thumb on image size:\n",
    "\n",
    "* With filter size $n$, $n$ odd, if we use padding $\\lfloor \\frac{n}{2} \\rfloor$ and stride 1, then we get the same output size as input size.\n",
    "* With filter size $n$, $n$ even, if we use padding $\\frac{n}{2}$ and stride 1, then we get the same output size as input size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [`conv2dtranspose`](http://pytorch.org/docs/stable/nn.html#torch.nn.ConvTranspose2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `inputs`: (`batch_size`, `in_channels`, `size_h`, `size_w`)\n",
    "* `filters`: (`out_channels`, `in_channels`, `size_h`, `size_w`)\n",
    "\n",
    "Default: \n",
    "\n",
    "* No padding\n",
    "* Stride 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With square kernels and equal stride\n",
    "m1 = nn.ConvTranspose2d(16, 33, 3, stride=2)\n",
    "input1 = torch.randn(20, 16, 50, 100)\n",
    "output1 = m1(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 33, 101, 201])\n"
     ]
    }
   ],
   "source": [
    "print(output1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: \n",
    "\n",
    "1. We want an input size that will have us end up with batch size 20, with 16 channels, height 50, and width 100, if we apply to it a convolution operation 16 output channels, 33 input channels, filter size 3 and stride 2.\n",
    "2. It turns out to do this, we can start with batch size 20, 33 channels (both obvious), and, because of the geometry of the convolution, height 101 and width 201."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rules of thumb**:\n",
    "\n",
    "Similar rules of thumb apply: if we want to add deconvolution layers to a network to make it deeper, adding `ConvTranspose2d` operations with filter size $n$ and padding $\\lfloor \\frac{n}{2} \\rfloor$ can make your network deeper without messing with your image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input2 = torch.randn(1, 16, 28, 28)\n",
    "m2 = nn.ConvTranspose2d(16, 32, 3, padding=1)\n",
    "output2 = m2(input2)\n",
    "print(output2.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
