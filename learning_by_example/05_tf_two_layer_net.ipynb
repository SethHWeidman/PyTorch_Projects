{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TensorFlow: Static Graphs\n",
    "-------------------------\n",
    "\n",
    "A fully-connected ReLU network with one hidden layer and no biases, trained to\n",
    "predict y from x by minimizing squared Euclidean distance.\n",
    "\n",
    "This implementation uses basic TensorFlow operations to set up a computational\n",
    "graph, then executes the graph many times to actually train the network.\n",
    "\n",
    "One of the main differences between TensorFlow and PyTorch is that TensorFlow\n",
    "uses static computational graphs while PyTorch uses dynamic computational\n",
    "graphs.\n",
    "\n",
    "In TensorFlow we first set up the computational graph, then execute the same\n",
    "graph many times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37150330.0\n",
      "35839652.0\n",
      "35671600.0\n",
      "30922864.0\n",
      "21490744.0\n",
      "12222178.0\n",
      "6287355.0\n",
      "3364257.5\n",
      "2046526.2\n",
      "1426296.1\n",
      "1093109.6\n",
      "884464.4\n",
      "736646.06\n",
      "623493.0\n",
      "533146.5\n",
      "459286.56\n",
      "398111.3\n",
      "346897.75\n",
      "303665.6\n",
      "266918.6\n",
      "235553.08\n",
      "208599.06\n",
      "185312.06\n",
      "165112.97\n",
      "147531.58\n",
      "132156.95\n",
      "118692.445\n",
      "106842.516\n",
      "96378.836\n",
      "87133.5\n",
      "78934.16\n",
      "71636.66\n",
      "65132.355\n",
      "59322.098\n",
      "54115.047\n",
      "49444.883\n",
      "45243.72\n",
      "41454.773\n",
      "38034.14\n",
      "34939.35\n",
      "32133.047\n",
      "29584.607\n",
      "27268.559\n",
      "25162.453\n",
      "23247.984\n",
      "21500.14\n",
      "19901.766\n",
      "18438.525\n",
      "17099.383\n",
      "15871.17\n",
      "14743.01\n",
      "13705.85\n",
      "12750.848\n",
      "11871.532\n",
      "11061.183\n",
      "10313.422\n",
      "9622.469\n",
      "8983.492\n",
      "8392.522\n",
      "7846.0996\n",
      "7339.205\n",
      "6868.945\n",
      "6432.494\n",
      "6027.0986\n",
      "5650.596\n",
      "5300.0835\n",
      "4973.841\n",
      "4669.8223\n",
      "4386.4614\n",
      "4122.3945\n",
      "3875.9656\n",
      "3645.8235\n",
      "3430.8164\n",
      "3229.976\n",
      "3042.3955\n",
      "2866.9966\n",
      "2702.702\n",
      "2548.7827\n",
      "2404.6155\n",
      "2269.6335\n",
      "2143.0586\n",
      "2024.2738\n",
      "1912.8217\n",
      "1808.1282\n",
      "1709.749\n",
      "1617.2769\n",
      "1530.3069\n",
      "1448.4907\n",
      "1371.4744\n",
      "1298.9791\n",
      "1230.7698\n",
      "1166.4623\n",
      "1105.8154\n",
      "1048.6389\n",
      "994.7019\n",
      "943.8037\n",
      "895.75085\n",
      "850.4005\n",
      "807.5541\n",
      "767.0704\n",
      "728.7974\n",
      "692.61\n",
      "658.39355\n",
      "626.02966\n",
      "595.3868\n",
      "566.37524\n",
      "538.89545\n",
      "512.8673\n",
      "488.2018\n",
      "464.83322\n",
      "442.6889\n",
      "421.6771\n",
      "401.73877\n",
      "382.82852\n",
      "364.87927\n",
      "347.8444\n",
      "331.66058\n",
      "316.29672\n",
      "301.71112\n",
      "287.8409\n",
      "274.66187\n",
      "262.12863\n",
      "250.21588\n",
      "238.87944\n",
      "228.0911\n",
      "217.83029\n",
      "208.06723\n",
      "198.77122\n",
      "189.91531\n",
      "181.48074\n",
      "173.44434\n",
      "165.79059\n",
      "158.4925\n",
      "151.5441\n",
      "144.9184\n",
      "138.59624\n",
      "132.56812\n",
      "126.82253\n",
      "121.33812\n",
      "116.1043\n",
      "111.11134\n",
      "106.34509\n",
      "101.795425\n",
      "97.44884\n",
      "93.298096\n",
      "89.33555\n",
      "85.5491\n",
      "81.93025\n",
      "78.47867\n",
      "75.18195\n",
      "72.028755\n",
      "69.01395\n",
      "66.13181\n",
      "63.375557\n",
      "60.739784\n",
      "58.219933\n",
      "55.808613\n",
      "53.50203\n",
      "51.294083\n",
      "49.182037\n",
      "47.160282\n",
      "45.225582\n",
      "43.37275\n",
      "41.599556\n",
      "39.901405\n",
      "38.27466\n",
      "36.71878\n",
      "35.22688\n",
      "33.797432\n",
      "32.43004\n",
      "31.11964\n",
      "29.863811\n",
      "28.66074\n",
      "27.506374\n",
      "26.400875\n",
      "25.34151\n",
      "24.326077\n",
      "23.352684\n",
      "22.41909\n",
      "21.523508\n",
      "20.665787\n",
      "19.843357\n",
      "19.054218\n",
      "18.297344\n",
      "17.571726\n",
      "16.875576\n",
      "16.207401\n",
      "15.566679\n",
      "14.951416\n",
      "14.3613405\n",
      "13.795625\n",
      "13.252908\n",
      "12.731257\n",
      "12.231496\n",
      "11.7516165\n",
      "11.290521\n",
      "10.847965\n",
      "10.423786\n",
      "10.015725\n",
      "9.624585\n",
      "9.249213\n",
      "8.888144\n",
      "8.541938\n",
      "8.209831\n",
      "7.8902583\n",
      "7.5837107\n",
      "7.289089\n",
      "7.0060415\n",
      "6.73437\n",
      "6.473554\n",
      "6.222976\n",
      "5.9821835\n",
      "5.7510247\n",
      "5.5288057\n",
      "5.3151574\n",
      "5.1101103\n",
      "4.9133162\n",
      "4.723611\n",
      "4.5418644\n",
      "4.3670387\n",
      "4.1993523\n",
      "4.037788\n",
      "3.8827531\n",
      "3.7336245\n",
      "3.5906463\n",
      "3.4530058\n",
      "3.3206425\n",
      "3.19326\n",
      "3.0712729\n",
      "2.9537194\n",
      "2.8407278\n",
      "2.7322454\n",
      "2.6278975\n",
      "2.5275435\n",
      "2.4312408\n",
      "2.3384514\n",
      "2.2494283\n",
      "2.1637597\n",
      "2.081258\n",
      "2.0021188\n",
      "1.9257989\n",
      "1.8526561\n",
      "1.7822335\n",
      "1.7145617\n",
      "1.6495064\n",
      "1.5868113\n",
      "1.5266378\n",
      "1.4687424\n",
      "1.4130127\n",
      "1.3594954\n",
      "1.3080372\n",
      "1.2585856\n",
      "1.2108684\n",
      "1.16504\n",
      "1.1208699\n",
      "1.0785346\n",
      "1.0377697\n",
      "0.9986231\n",
      "0.96078753\n",
      "0.9245074\n",
      "0.8896974\n",
      "0.856129\n",
      "0.8238176\n",
      "0.79274476\n",
      "0.7629565\n",
      "0.7342257\n",
      "0.7066132\n",
      "0.6799591\n",
      "0.6543831\n",
      "0.6298085\n",
      "0.60602397\n",
      "0.5831682\n",
      "0.561244\n",
      "0.54016954\n",
      "0.51997703\n",
      "0.5004191\n",
      "0.48157877\n",
      "0.46355614\n",
      "0.4462052\n",
      "0.42938823\n",
      "0.41327998\n",
      "0.3977527\n",
      "0.3828217\n",
      "0.3685133\n",
      "0.35468227\n",
      "0.34143814\n",
      "0.32868534\n",
      "0.3163095\n",
      "0.30454475\n",
      "0.29306757\n",
      "0.28210503\n",
      "0.27157736\n",
      "0.26140565\n",
      "0.25169903\n",
      "0.24226987\n",
      "0.23322515\n",
      "0.2244942\n",
      "0.21613336\n",
      "0.20804839\n",
      "0.20027778\n",
      "0.19281605\n",
      "0.18559004\n",
      "0.17865002\n",
      "0.17205672\n",
      "0.16561154\n",
      "0.15941434\n",
      "0.1534761\n",
      "0.1477645\n",
      "0.14224967\n",
      "0.13696422\n",
      "0.13183099\n",
      "0.12691034\n",
      "0.122191906\n",
      "0.117649086\n",
      "0.113298856\n",
      "0.109089166\n",
      "0.10502586\n",
      "0.101113275\n",
      "0.097354755\n",
      "0.09373509\n",
      "0.090249255\n",
      "0.086916685\n",
      "0.08370362\n",
      "0.08060054\n",
      "0.07760139\n",
      "0.074729756\n",
      "0.071936764\n",
      "0.06929706\n",
      "0.066723704\n",
      "0.0642514\n",
      "0.061857387\n",
      "0.059600256\n",
      "0.057381805\n",
      "0.05524674\n",
      "0.053192932\n",
      "0.051234514\n",
      "0.04933551\n",
      "0.047507565\n",
      "0.045749955\n",
      "0.044058345\n",
      "0.042440187\n",
      "0.040874287\n",
      "0.039368715\n",
      "0.03793384\n",
      "0.036520075\n",
      "0.035174593\n",
      "0.03388197\n",
      "0.032612633\n",
      "0.031410947\n",
      "0.030277181\n",
      "0.029160775\n",
      "0.028083399\n",
      "0.0270568\n",
      "0.026054446\n",
      "0.02509541\n",
      "0.024181519\n",
      "0.023291092\n",
      "0.022446506\n",
      "0.02162066\n",
      "0.020823292\n",
      "0.020073766\n",
      "0.019346455\n",
      "0.018628633\n",
      "0.017945647\n",
      "0.017303007\n",
      "0.016667329\n",
      "0.01605701\n",
      "0.015470821\n",
      "0.014915794\n",
      "0.014370365\n",
      "0.013845788\n",
      "0.013340484\n",
      "0.012850552\n",
      "0.0123933675\n",
      "0.011944999\n",
      "0.011515036\n",
      "0.01110482\n",
      "0.0107098855\n",
      "0.010324535\n",
      "0.00996442\n",
      "0.009603638\n",
      "0.009262035\n",
      "0.008937438\n",
      "0.008621454\n",
      "0.008314882\n",
      "0.008024827\n",
      "0.007743042\n",
      "0.0074736355\n",
      "0.007208255\n",
      "0.0069503416\n",
      "0.006708598\n",
      "0.0064727645\n",
      "0.006244663\n",
      "0.006031668\n",
      "0.0058199833\n",
      "0.0056180237\n",
      "0.005424724\n",
      "0.0052453615\n",
      "0.005064609\n",
      "0.0048966324\n",
      "0.004728395\n",
      "0.004565378\n",
      "0.00441252\n",
      "0.00426078\n",
      "0.004115852\n",
      "0.003979631\n",
      "0.0038481588\n",
      "0.0037217261\n",
      "0.0035996828\n",
      "0.0034825844\n",
      "0.0033680107\n",
      "0.0032594288\n",
      "0.0031538573\n",
      "0.003049151\n",
      "0.0029509042\n",
      "0.0028578134\n",
      "0.0027644068\n",
      "0.0026787554\n",
      "0.002592182\n",
      "0.0025122613\n",
      "0.002436878\n",
      "0.0023589279\n",
      "0.0022818977\n",
      "0.0022108878\n",
      "0.0021440757\n",
      "0.0020781378\n",
      "0.0020168915\n",
      "0.0019573416\n",
      "0.0019000784\n",
      "0.0018414273\n",
      "0.0017878253\n",
      "0.0017343712\n",
      "0.0016843606\n",
      "0.0016329562\n",
      "0.0015881098\n",
      "0.001541299\n",
      "0.0014965936\n",
      "0.0014524825\n",
      "0.001412956\n",
      "0.0013723021\n",
      "0.0013335396\n",
      "0.0012985049\n",
      "0.001262347\n",
      "0.0012281921\n",
      "0.0011953481\n",
      "0.0011623683\n",
      "0.001129455\n",
      "0.001098189\n",
      "0.001070507\n",
      "0.0010423324\n",
      "0.0010155948\n",
      "0.0009892137\n",
      "0.00096234673\n",
      "0.0009386609\n",
      "0.0009151698\n",
      "0.0008910076\n",
      "0.0008681017\n",
      "0.0008463654\n",
      "0.0008259195\n",
      "0.0008048238\n",
      "0.00078572973\n",
      "0.000766105\n",
      "0.00074710697\n",
      "0.00072999933\n",
      "0.00071262947\n",
      "0.0006958718\n",
      "0.0006800898\n",
      "0.0006626844\n",
      "0.00064805045\n",
      "0.0006323852\n",
      "0.0006175983\n",
      "0.0006037216\n",
      "0.0005893305\n",
      "0.00057675113\n",
      "0.0005640419\n",
      "0.00055050524\n",
      "0.00053811946\n",
      "0.0005262544\n",
      "0.00051552826\n",
      "0.0005046982\n",
      "0.00049329357\n",
      "0.0004824867\n",
      "0.00047204108\n",
      "0.0004625157\n",
      "0.0004531405\n",
      "0.00044325704\n",
      "0.00043458107\n",
      "0.0004249986\n",
      "0.00041654633\n",
      "0.00040789525\n",
      "0.00039855778\n",
      "0.00039100705\n",
      "0.00038317707\n",
      "0.0003751741\n",
      "0.0003683176\n",
      "0.00036139673\n",
      "0.00035380461\n",
      "0.00034650805\n",
      "0.000339709\n",
      "0.00033267148\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# First we set up the computational graph:\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create placeholders for the input and target data; these will be filled\n",
    "# with real data when we execute the graph.\n",
    "x = tf.placeholder(tf.float32, shape=(None, D_in))\n",
    "y = tf.placeholder(tf.float32, shape=(None, D_out))\n",
    "\n",
    "# Create Variables for the weights and initialize them with random data.\n",
    "# A TensorFlow Variable persists its value across executions of the graph.\n",
    "w1 = tf.Variable(tf.random_normal((D_in, H)))\n",
    "w2 = tf.Variable(tf.random_normal((H, D_out)))\n",
    "\n",
    "# Forward pass: Compute the predicted y using operations on TensorFlow Tensors.\n",
    "# Note that this code does not actually perform any numeric operations; it\n",
    "# merely sets up the computational graph that we will later execute.\n",
    "h = tf.matmul(x, w1)\n",
    "h_relu = tf.maximum(h, tf.zeros(1))\n",
    "y_pred = tf.matmul(h_relu, w2)\n",
    "\n",
    "# Compute loss using operations on TensorFlow Tensors\n",
    "loss = tf.reduce_sum((y - y_pred) ** 2.0)\n",
    "\n",
    "# Compute gradient of the loss with respect to w1 and w2.\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "\n",
    "# Update the weights using gradient descent. To actually update the weights\n",
    "# we need to evaluate new_w1 and new_w2 when executing the graph. Note that\n",
    "# in TensorFlow the the act of updating the value of the weights is part of\n",
    "# the computational graph; in PyTorch this happens outside the computational\n",
    "# graph.\n",
    "learning_rate = 1e-6\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
    "\n",
    "# Now we have built our computational graph, so we enter a TensorFlow session to\n",
    "# actually execute the graph.\n",
    "with tf.Session() as sess:\n",
    "    # Run the graph once to initialize the Variables w1 and w2.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Create numpy arrays holding the actual data for the inputs x and targets\n",
    "    # y\n",
    "    x_value = np.random.randn(N, D_in)\n",
    "    y_value = np.random.randn(N, D_out)\n",
    "    for _ in range(500):\n",
    "        # Execute the graph many times. Each time it executes we want to bind\n",
    "        # x_value to x and y_value to y, specified with the feed_dict argument.\n",
    "        # Each time we execute the graph we want to compute the values for loss,\n",
    "        # new_w1, and new_w2; the values of these Tensors are returned as numpy\n",
    "        # arrays.\n",
    "        loss_value, _, _ = sess.run([loss, new_w1, new_w2],\n",
    "                                    feed_dict={x: x_value, y: y_value})\n",
    "        print(loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
